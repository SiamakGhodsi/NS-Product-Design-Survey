{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fO9a2K8QPEbt","executionInfo":{"status":"ok","timestamp":1769010408941,"user_tz":-60,"elapsed":22188,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"}},"outputId":"b6b9dd00-294e-4e48-9606-1b9dc5c7d31c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# Print the current working directory\n","print(\"Current directory:\", os.getcwd())\n","\n","# Change directory to your target folder on Google Drive\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/Survey Paper/Neuro-Symbolic Multi-objective RL/\")\n","print(\"New directory:\", os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cUqUZbDQPJMD","executionInfo":{"status":"ok","timestamp":1769010431753,"user_tz":-60,"elapsed":1645,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"}},"outputId":"e5cb1d61-1e2c-449f-a841-c87f5a9ff959"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Current directory: /content\n","New directory: /content/drive/My Drive/Colab Notebooks/Survey Paper/Neuro-Symbolic Multi-objective RL\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1361,"status":"ok","timestamp":1769010448673,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"},"user_tz":-60},"id":"0dMDGc_Orq-8"},"outputs":[],"source":["# ===== 0) Setup + config =====\n","from __future__ import annotations\n","\n","import os, sys, re, json, time, hashlib\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","from pathlib import Path\n","\n","import requests\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# --- Reproducible folders (repo-relative) ---\n","REPO_ROOT = Path(\".\")  # in Colab set to your mounted repo folder\n","DATA_RAW = REPO_ROOT / \"data\" / \"raw\"\n","DATA_INTERIM = REPO_ROOT / \"data\" / \"interim\"\n","DATA_PROCESSED = REPO_ROOT / \"data\" / \"processed\"\n","OUTPUTS_FIG = REPO_ROOT / \"outputs\" / \"figures\"\n","OUTPUTS_TABLES = REPO_ROOT / \"outputs\" / \"tables\"\n","SCREENING_DIR = REPO_ROOT / \"screening\"\n","BIB_DIR = REPO_ROOT / \"bib\"\n","\n","for p in [DATA_RAW, DATA_INTERIM, DATA_PROCESSED, OUTPUTS_FIG, OUTPUTS_TABLES, SCREENING_DIR, BIB_DIR]:\n","    p.mkdir(parents=True, exist_ok=True)\n","\n","# --- Global knobs ---\n","YEAR_THRESHOLD = 1980\n","\n","USE_VENUE_FILTER = False  # recommended OFF for product-design cross-domain surveys\n"]},{"cell_type":"markdown","metadata":{"id":"ud5nZUzGKeHe"},"source":["# Venues: Conf, Journal\n","Top 120 CS journal-conference collection\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"FW3--tEzY5K7","executionInfo":{"status":"ok","timestamp":1769010452024,"user_tz":-60,"elapsed":82,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"}}},"outputs":[],"source":["# Define a mapping of DBLP venue variants for our target conferences and journals\n","VENUE_VARIANTS = {\n","    \"ICML\": [\"ICML\", \"International Conference on Machine Learning\"],\n","    \"NIPS\": [\"NIPS\", \"NeurIPS\", \"Advances in Neural Information Processing Systems\"],\n","    \"ICLR\": [\"ICLR\", \"International Conference on Learning Representations\"],\n","    \"AAAI\": [\"AAAI\", \"AAAI Conference on Artificial Intelligence\"],\n","    \"IJCAI\": [\"IJCAI\", \"International Joint Conference on Artificial Intelligence\"],\n","    \"COLT\": [\"COLT\", \"Conference on Learning Theory\"],\n","    \"AISTATS\": [\"AISTATS\", \"International Conference on Artificial Intelligence and Statistics\"],\n","    \"JMLR\": [\"JMLR\", \"Journal of Machine Learning Research\"],\n","    \"Machine Learning\": [\"Machine Learning\"],\n","    \"JAIR\": [\"JAIR\", \"Journal of Artificial Intelligence Research\"],\n","    \"Frontiers in AI\": [\"Frontiers in AI\", \"Frontiers in Artificial Intelligence\"],\n","    \"AI Magazine\": [\"AI Magazine\"],\n","    \"KDD\": [\"KDD\", \"Knowledge Discovery and Data Mining\"],\n","    \"SIGIR\": [\"SIGIR\", \"ACM SIGIR Conference on Research and Development in Information Retrieval\"],\n","    \"CIKM\": [\"CIKM\", \"ACM International Conference on Information and Knowledge Management\"],\n","    \"ICDM\": [\"ICDM\", \"IEEE International Conference on Data Mining\"],\n","    \"ECMLPKDD\": [\"ECMLPKDD\", \"European Conference on Machine Learning\", \"Principles and Practice of Knowledge Discovery in Databases\"],\n","    \"DMKD\": [\"DMKD\", \"Data Mining and Knowledge Discovery\"],\n","    \"ACM TKDD\": [\"ACM TKDD\", \"ACM Transactions on Knowledge Discovery from Data\"],\n","    \"DKE\": [\"DKE\", \"Data & Knowledge Engineering\"],\n","    \"BDR\": [\"BDR\", \"Big Data Research\"],\n","    \"BigData\": [\"IEEE BigData\", \"IEEE International Conference on Big Data\"],\n","    \"CVPR\": [\"CVPR\", \"IEEE Conference on Computer Vision and Pattern Recognition\"],\n","    \"ICCV\": [\"ICCV\", \"International Conference on Computer Vision\"],\n","    \"ECCV\": [\"ECCV\", \"European Conference on Computer Vision\"],\n","    \"BMVC\": [\"BMVC\", \"British Machine Vision Conference\"],\n","    \"IJCV\": [\"IJCV\", \"International Journal of Computer Vision\"],\n","    \"CVIU\": [\"CVIU\", \"Computer Vision and Image Understanding\"],\n","    \"TIP\": [\"TIP\", \"IEEE Transactions on Image Processing\"],\n","    \"TSP\": [\"TSP\", \"IEEE Transactions on Signal Processing\"],\n","    \"IEEE TCSI\": [\"IEEE TCSI\", \"IEEE Transactions on Circuits and Systems for Video Technology\"],\n","    \"ACL\": [\"ACL\", \"Annual Meeting of the Association for Computational Linguistics\"],\n","    \"NAACL\": [\"NAACL\", \"North American Chapter of the Association for Computational Linguistics\"],\n","    \"EMNLP\": [\"EMNLP\", \"Conference on Empirical Methods in Natural Language Processing\"],\n","    \"COLING\": [\"COLING\", \"International Conference on Computational Linguistics\"],\n","    \"RECSYS\": [\"RECSYS\", \"ACM Conference on Recommender Systems\"],\n","    \"CHI\": [\"CHI\", \"Conference on Human Factors in Computing Systems\"],\n","    \"PACMHCI\": [\"PACMHCI\", \"Proceedings of the ACM on Human-Computer Interaction\"],\n","    \"UbiComp\": [\"UbiComp\", \"ACM International Joint Conference on Pervasive and Ubiquitous Computing\"],\n","    \"MobiCom\": [\"MobiCom\", \"ACM International Conference on Mobile Computing and Networking\"],\n","    \"MobiSys\": [\"MobiSys\", \"International Conference on Mobile Systems, Applications, and Services\"],\n","    \"WWW\": [\"WWW\", \"World Wide Web\", \"The Web Conference\"],\n","    \"WSDM\": [\"WSDM\", \"Web Search and Data Mining\"],\n","    \"ICWSM\": [\"ICWSM\", \"International Conference on Web and Social Media\"],\n","    \"ICRA\": [\"ICRA\", \"IEEE International Conference on Robotics and Automation\"],\n","    \"IROS\": [\"IROS\", \"IEEE/RSJ International Conference on Intelligent Robots and Systems\"],\n","    \"RSS\": [\"RSS\", \"Robotics: Science and Systems\"],\n","    \"ACM TOIS\": [\"ACM TOIS\", \"ACM Transactions on Information Systems\"],\n","    \"ACM CSUR\": [\"ACM CSUR\", \"ACM Computing Surveys\"],\n","    \"TOCS\": [\"TOCS\", \"ACM Transactions on Computer Systems\"],\n","    \"TOMS\": [\"TOMS\", \"ACM Transactions on Mathematical Software\"],\n","    \"ACM TODS\": [\"ACM TODS\", \"ACM Transactions on Database Systems\"],\n","    \"IS\": [\"IS\", \"Information Systems\"],\n","    \"DSS\": [\"DSS\", \"Decision Support Systems\"],\n","    \"MISQ\": [\"MISQ\", \"MIS Quarterly\"],\n","    \"ISR\": [\"ISR\", \"Information Systems Research\"],\n","    \"JMIS\": [\"JMIS\", \"Journal of Management Information Systems\"],\n","    \"JASIST\": [\"JASIST\", \"Journal of the Association for Information Science and Technology\"],\n","    \"IEEE TPAMI\": [\"IEEE TPAMI\", \"IEEE Transactions on Pattern Analysis and Machine Intelligence\"],\n","    \"IEEE TKDE\": [\"IEEE TKDE\", \"IEEE Transactions on Knowledge and Data Engineering\"],\n","    \"IEEE TNNLS\": [\"IEEE TNNLS\", \"IEEE Transactions on Neural Networks and Learning Systems\"],\n","    \"IEEE TC\": [\"IEEE TC\", \"IEEE Transactions on Cybernetics\"],\n","    \"IEEE TETC\": [\"IEEE TETC\", \"IEEE Transactions on Emerging Topics in Computational Intelligence\"],\n","    \"IEEE TFS\": [\"IEEE TFS\", \"IEEE Transactions on Fuzzy Systems\"],\n","    \"IEEE TEC\": [\"IEEE TEC\", \"IEEE Transactions on Evolutionary Computation\"],\n","    \"IEEE Intelligent Systems\": [\"IEEE Intelligent Systems\"],\n","    \"IEEE Access\": [\"IEEE Access\"],\n","    \"FSS\": [\"FSS\", \"Fuzzy Sets and Systems\"],\n","    \"Swarm Intelligence\": [\"Swarm Intelligence\"],\n","    \"ALife\": [\"ALife\", \"Artificial Life\"],\n","    \"EC\": [\"EC\", \"Evolutionary Computation\"],\n","    \"ACM TIST\": [\"ACM TIST\", \"ACM Transactions on Intelligent Systems and Technology\"],\n","    \"ACM TORS\": [\"ACM TORS\", \"ACM Transactions on Recommender Systems\"],\n","    \"ACM TOMM\": [\"ACM TOMM\", \"ACM Transactions on Multimedia Computing, Communications, and Applications\"],\n","    \"Neurocomputing\": [\"Neurocomputing\"],\n","    \"KAIS\": [\"KAIS\", \"Knowledge and Information Systems\"],\n","    \"IPM\": [\"IPM\", \"Information Processing & Management\"],\n","    \"Information Fusion\": [\"Information Fusion\"],\n","    \"PRL\": [\"PRL\", \"Pattern Recognition Letters\"],\n","    \"PAA\": [\"PAA\", \"Pattern Analysis and Applications\"],\n","    \"IJDSA\": [\"IJDSA\", \"International Journal of Data Science and Analytics\"],\n","    \"TVCG\": [\"TVCG\", \"IEEE Transactions on Visualization and Computer Graphics\"],\n","    \"SIGGRAPH\": [\"SIGGRAPH\", \"ACM SIGGRAPH Conference\"],\n","    \"IUI\": [\"IUI\", \"ACM International Conference on Intelligent User Interfaces\"],\n","    \"HRI\": [\"HRI\", \"ACM/IEEE International Conference on Human-Robot Interaction\"],\n","    \"ICMI\": [\"ICMI\", \"International Conference on Multimodal Interaction\"],\n","    \"ICMLA\": [\"ICMLA\", \"International Conference on Machine Learning and Applications\"],\n","    \"PACMHCI\": [\"PACMHCI\", \"Proceedings of the ACM on Human-Computer Interaction\"],\n","    \"IJCNN\": [\"IJCNN\", \"International Joint Conference on Neural Networks\"],\n","    \"FAccT\": [\"FAccT\", \"ACM Conference on Fairness, Accountability, and Transparency\"],\n","    \"EAAI\": [\"EAAI\", \"Engineering Applications of Artificial Intelligence\"],\n","    \"ACM SIGMOD\": [\"ACM SIGMOD\", \"ACM SIGMOD International Conference on Management of Data\"],\n","    \"IET\": [\"IET\", \"IET Intelligent Transport Systems\"],\n","    \"IV\": [\"IV\", \"Intelligent Vehicles\"],\n","    \"SAS\": [\"SAS\", \"IEEE/ACM International Conference on Social Computing\"],\n","    \"ICIS\": [\"ICIS\", \"International Conference on Information Systems\"],\n","    \"ECIS\": [\"ECIS\", \"European Conference on Information Systems\"],\n","    \"Nature\": [\"Nature\"],\n","    \"Science\": [\"Science\"],\n","    \"Nature Machine Intelligence\": [\"Nature Machine Intelligence\"],\n","    \"Nature Methods\": [\"Nature Methods\"],\n","    \"Nature Biotechnology\": [\"Nature Biotechnology\"],\n","    \"Nature Communications\": [\"Nature Communications\"],\n","    \"PNAS\": [\"PNAS\", \"Proceedings of the National Academy of Sciences\"],\n","    # Additional Emerging & Specialized Venues (Springer, Elsevier, MDPI, IOS Press, etc.)\n","    \"Engineering Applications of AI\": [\"Engineering Applications of Artificial Intelligence\", \"EAAI\"],\n","    \"Expert Systems\": [\"Expert Systems\", \"Expert Systems with Applications\"],\n","    \"IOS Press\": [\"IOS Press\"],  # Placeholder for IOS Press titles\n","    \"MDPI Sensors\": [\"MDPI Sensors\"],\n","    \"Springer Journal of Data Science\": [\"Springer Journal of Data Science\"],\n","    \"ACM TCPS\": [\"ACM TCPS\", \"ACM Transactions on Cyber-Physical Systems\"],\n","    \"IJI\": [\"IJI\", \"International Journal of Intelligent Systems\"],\n","    \"Frontiers in Neuroscience\": [\"Frontiers in Neuroscience\"],\n","    \"Frontiers in Robotics and AI\": [\"Frontiers in Robotics and AI\"],\n","    # Applied, industrial, and design venues\n","    \"IEEE IT\": [\"IEEE IT\", \"IEEE International Conference on Industrial Technology\"],\n","    \"IEEM\": [\"IEEM\", \"IEEE International Conference on Industrial Engineering and Engineering Management\"],\n","    \"CIRP\": [\"CIRP\", \"CIRP Annals\", \"CIRP Annals - Manufacturing Technology\"],\n","    \"IJPR\": [\"IJPR\", \"International Journal of Production Research\"],\n","    \"JMS\": [\"JMS\", \"Journal of Manufacturing Systems\"],\n","    \"CI\": [\"CI\", \"Computers in Industry\"],\n","    \"IEEE TASE\": [\"IEEE TASE\", \"IEEE Transactions on Automation Science and Engineering\"],\n","    \"DS\": [\"DS\", \"Design Studies\"],\n","    \"IJD\": [\"IJD\", \"International Journal of Design\"],\n","    \"IPE\": [\"IPE\", \"International Journal of Production Economics\"],\n","    \"JIM\": [\"JIM\", \"Journal of Intelligent Manufacturing\"],\n","    \"JCP\": [\"JCP\", \"Journal of Cleaner Production\"],\n","    \"RCR\": [\"RCR\", \"Resources, Conservation and Recycling\"],\n","    \"EJOR\": [\"EJOR\", \"European Journal of Operational Research\"],\n","    \"CIE\": [\"CIE\", \"Computers & Industrial Engineering\"],\n","    \"PPC\": [\"PPC\", \"Production Planning & Control\"]\n","}"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NrUVRFcDZHYI","executionInfo":{"status":"ok","timestamp":1769010466333,"user_tz":-60,"elapsed":54,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"}}},"outputs":[],"source":["# ===== 1) Helpers: normalization + IDs =====\n","\n","def normalize_whitespace(s: str) -> str:\n","    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n","\n","def normalize_title(s: str) -> str:\n","    s = normalize_whitespace(s).lower()\n","    # remove latex/braces and punctuation-ish\n","    s = re.sub(r\"[{}$\\\\]\", \"\", s)\n","    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","def extract_doi(text: str) -> Optional[str]:\n","    if not text:\n","        return None\n","    # permissive DOI regex\n","    m = re.search(r\"\\b10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+\\b\", text)\n","    return m.group(0).lower() if m else None\n","\n","def safe_int(x) -> Optional[int]:\n","    try:\n","        return int(x)\n","    except Exception:\n","        return None\n","\n","def make_paper_id(doi: Optional[str], arxiv_id: Optional[str], s2_id: Optional[str],\n","                  title: str, year: Optional[int]) -> str:\n","    \"\"\"\n","    Stable ID priority:\n","    DOI > arXiv > SemanticScholar paperId > normalized title+year\n","    \"\"\"\n","    if doi:\n","        return f\"doi:{doi}\"\n","    if arxiv_id:\n","        return f\"arxiv:{arxiv_id.lower()}\"\n","    if s2_id:\n","        return f\"s2:{s2_id}\"\n","    key = f\"{normalize_title(title)}::{year or ''}\"\n","    return \"tpy:\" + hashlib.sha1(key.encode(\"utf-8\")).hexdigest()[:16]\n"]},{"cell_type":"markdown","source":["### 2) Retrieval: DBLP"],"metadata":{"id":"WI_YDZvTNRCr"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZJpw7T6ZFhqa","executionInfo":{"status":"ok","timestamp":1769010473084,"user_tz":-60,"elapsed":40,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"}}},"outputs":[],"source":["# ===== 2) Retrieval: DBLP =====\n","\n","DBLP_SEARCH_URL = \"https://dblp.org/search/publ/api\"\n","\n","def dblp_search(query: str, start: int = 0, hits: int = 100) -> List[dict]:\n","    params = {\"q\": query, \"format\": \"json\", \"h\": hits, \"f\": start}\n","    r = requests.get(DBLP_SEARCH_URL, params=params, timeout=60)\n","    r.raise_for_status()\n","    data = r.json()\n","    return data.get(\"result\", {}).get(\"hits\", {}).get(\"hit\", []) or []\n","\n","def dblp_fetch_all(query: str, hits: int = 100, max_pages: int = 5, sleep_s: float = 1.0) -> List[dict]:\n","    out = []\n","    start = 0\n","    for _ in range(max_pages):\n","        batch = dblp_search(query, start=start, hits=hits)\n","        if not batch:\n","            break\n","        out.extend(batch)\n","        start += hits\n","        time.sleep(sleep_s)\n","    return out\n","\n","def dblp_get_bibtex(dblp_url: str) -> Optional[str]:\n","    if not dblp_url:\n","        return None\n","    bib_url = dblp_url + \".bib\"\n","    r = requests.get(bib_url, timeout=60)\n","    if r.status_code == 200 and r.text.strip().startswith(\"@\"):\n","        return r.text\n","    return None\n","\n","def parse_dblp_hit(hit: dict) -> dict:\n","    info = hit.get(\"info\", {}) or {}\n","    title = normalize_whitespace(info.get(\"title\", \"\"))\n","    year = safe_int(info.get(\"year\"))\n","    venue = normalize_whitespace(info.get(\"venue\", \"\")) if not isinstance(info.get(\"venue\"), list) else normalize_whitespace(\" \".join(info.get(\"venue\")))\n","    url = normalize_whitespace(info.get(\"url\", \"\"))\n","\n","    # DBLP doesn't reliably provide DOI in the json hit; try DOI from bibtex later\n","    return {\n","        \"source\": \"DBLP\",\n","        \"title\": title,\n","        \"year\": year,\n","        \"venue\": venue,\n","        \"url\": url,\n","        \"abstract\": None,\n","        \"doi\": None,\n","        \"arxiv_id\": None,\n","        \"s2_id\": None,\n","        \"dblp_bibtex\": None,\n","    }\n","\n","def retrieve_from_dblp(queries: Dict[str, List[str]], max_pages: int = 5) -> pd.DataFrame:\n","    rows = []\n","    for topic, qlist in queries.items():\n","        for q in qlist:\n","            hits = dblp_fetch_all(q, hits=100, max_pages=max_pages, sleep_s=1.0)\n","            for h in hits:\n","                row = parse_dblp_hit(h)\n","                row[\"topic_bucket\"] = topic\n","                row[\"query\"] = q\n","                # basic filters\n","                if row[\"year\"] is not None and row[\"year\"] < YEAR_THRESHOLD:\n","                    continue\n","                if not row[\"title\"]:\n","                    continue\n","\n","                # fetch bibtex (optional but useful for DOI)\n","                if row[\"url\"]:\n","                    bib = dblp_get_bibtex(row[\"url\"])\n","                    row[\"dblp_bibtex\"] = bib\n","                    # try extract DOI from bibtex\n","                    if bib:\n","                        doi = extract_doi(bib)\n","                        if doi:\n","                            row[\"doi\"] = doi\n","                rows.append(row)\n","    df = pd.DataFrame(rows).drop_duplicates(subset=[\"source\", \"title\", \"year\", \"venue\", \"url\", \"query\"])\n","    return df\n"]},{"cell_type":"markdown","source":["## 3) Retrieval: Semantic Scholar"],"metadata":{"id":"BX0COybTNWQG"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"5KfY2ag-ce7f","executionInfo":{"status":"ok","timestamp":1769010480976,"user_tz":-60,"elapsed":31,"user":{"displayName":"Siamak Ghodsi","userId":"13068637638907409876"}}},"outputs":[],"source":["# ===== 3) Retrieval: Semantic Scholar =====\n","\n","S2_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n","\n","def s2_search(query: str, offset: int = 0, limit: int = 100, fields: str = None,\n","              max_retries: int = 6, sleep_s: float = 2.0) -> List[dict]:\n","    if fields is None:\n","        fields = \"paperId,title,year,venue,abstract,url,externalIds,publicationTypes\"\n","    params = {\"query\": query, \"offset\": offset, \"limit\": limit, \"fields\": fields}\n","    for i in range(max_retries):\n","        r = requests.get(S2_URL, params=params, timeout=60)\n","        if r.status_code == 200:\n","            return r.json().get(\"data\", []) or []\n","        if r.status_code == 429:\n","            time.sleep(sleep_s * (i + 1))\n","            continue\n","        # other errors: stop\n","        raise RuntimeError(f\"S2 API error {r.status_code}: {r.text[:200]}\")\n","    return []\n","\n","def s2_fetch_all(query: str, limit: int = 100, max_pages: int = 3) -> List[dict]:\n","    out, offset = [], 0\n","    for _ in range(max_pages):\n","        batch = s2_search(query, offset=offset, limit=limit)\n","        if not batch:\n","            break\n","        out.extend(batch)\n","        offset += limit\n","        time.sleep(1.0)\n","    return out\n","\n","def parse_s2_entry(e: dict) -> dict:\n","    title = normalize_whitespace(e.get(\"title\", \"\"))\n","    year = safe_int(e.get(\"year\"))\n","    venue = normalize_whitespace(e.get(\"venue\", \"\"))\n","    abstract = normalize_whitespace(e.get(\"abstract\", \"\")) or None\n","    url = normalize_whitespace(e.get(\"url\", \"\"))\n","    s2_id = e.get(\"paperId\")\n","\n","    external = e.get(\"externalIds\", {}) or {}\n","    doi = external.get(\"DOI\")\n","    if doi:\n","        doi = doi.lower()\n","    arxiv_id = external.get(\"ArXiv\")\n","\n","    return {\n","        \"source\": \"SemanticScholar\",\n","        \"title\": title,\n","        \"year\": year,\n","        \"venue\": venue,\n","        \"url\": url,\n","        \"abstract\": abstract,\n","        \"doi\": doi,\n","        \"arxiv_id\": arxiv_id,\n","        \"s2_id\": s2_id,\n","        \"dblp_bibtex\": None,\n","    }\n","\n","def retrieve_from_s2(queries: Dict[str, List[str]], max_pages: int = 3) -> pd.DataFrame:\n","    rows = []\n","    for topic, qlist in queries.items():\n","        for q in qlist:\n","            results = s2_fetch_all(q, limit=100, max_pages=max_pages)\n","            for e in results:\n","                row = parse_s2_entry(e)\n","                row[\"topic_bucket\"] = topic\n","                row[\"query\"] = q\n","                if row[\"year\"] is not None and row[\"year\"] < YEAR_THRESHOLD:\n","                    continue\n","                if not row[\"title\"]:\n","                    continue\n","                rows.append(row)\n","    df = pd.DataFrame(rows).drop_duplicates(subset=[\"source\", \"s2_id\", \"title\", \"year\", \"venue\", \"query\"])\n","    return df\n"]},{"cell_type":"markdown","metadata":{"id":"jLz2ywEtLNAd"},"source":["## 4) Stage 1 Run: define queries + pull + save raw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3xFeNEikgge"},"outputs":[],"source":["# ===== 4) Stage 1 run: define query buckets + retrieve =====\n","\n","# Search specifically for TypeVI: Differentiable + Shielding\n","query_buckets = {\n","    \"NS+Design\": [\n","        \"neuro-symbolic product design\",\n","        \"neurosymbolic CAD generation constraints\",\n","        \"neuro-symbolic manufacturing optimization\",\n","        \"neuro-symbolic assembly planning\"\n","    ],\n","    \"TypeVI_differentiable_solvers\": [\n","        \"differentiable optimization layer OptNet\",\n","        \"CVXPYLayers differentiable convex optimization layer\",\n","        \"SATNet differentiable satisfiability\"\n","    ],\n","    \"Safety_Shielding\": [\n","        \"reinforcement learning shielding temporal logic\",\n","        \"safe reinforcement learning shield\"\n","    ],\n","}\n","\n","df_dblp = retrieve_from_dblp(query_buckets, max_pages=3)\n","df_s2 = retrieve_from_s2(query_buckets, max_pages=3)\n","\n","raw_path_dblp = DATA_RAW / \"candidates_dblp_raw.csv\"\n","raw_path_s2 = DATA_RAW / \"candidates_s2_raw.csv\"\n","\n","df_dblp.to_csv(raw_path_dblp, index=False)\n","df_s2.to_csv(raw_path_s2, index=False)\n","\n","print(\"DBLP raw:\", len(df_dblp), \"->\", raw_path_dblp)\n","print(\"S2 raw:\", len(df_s2), \"->\", raw_path_s2)\n"]},{"cell_type":"markdown","metadata":{"id":"03NjBbJgJrkT"},"source":["\n","## 5) Normalize + merge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72l95b0WYuXc"},"outputs":[],"source":["# ===== 5) Stage 2: normalize + merge =====\n","\n","def unify_schema(df: pd.DataFrame) -> pd.DataFrame:\n","    # ensure all columns exist\n","    cols = [\"source\",\"title\",\"year\",\"venue\",\"url\",\"abstract\",\"doi\",\"arxiv_id\",\"s2_id\",\"dblp_bibtex\",\"topic_bucket\",\"query\"]\n","    for c in cols:\n","        if c not in df.columns:\n","            df[c] = None\n","    df = df[cols].copy()\n","    # clean strings\n","    for c in [\"source\",\"title\",\"venue\",\"url\",\"abstract\",\"doi\",\"arxiv_id\",\"s2_id\",\"topic_bucket\",\"query\"]:\n","        df[c] = df[c].astype(str).replace({\"nan\": \"\"})\n","        df[c] = df[c].apply(lambda x: normalize_whitespace(x) if isinstance(x, str) else x)\n","        df[c] = df[c].replace({\"\": None})\n","    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n","    return df\n","\n","df_dblp_u = unify_schema(df_dblp)\n","df_s2_u = unify_schema(df_s2)\n","\n","df_merged = pd.concat([df_dblp_u, df_s2_u], ignore_index=True)\n","\n","# generate paper_id\n","df_merged[\"paper_id\"] = df_merged.apply(\n","    lambda r: make_paper_id(r[\"doi\"], r[\"arxiv_id\"], r[\"s2_id\"], r[\"title\"] or \"\", int(r[\"year\"]) if pd.notna(r[\"year\"]) else None),\n","    axis=1\n",")\n","\n","merged_path = DATA_INTERIM / \"candidates_merged.csv\"\n","df_merged.to_csv(merged_path, index=False)\n","print(\"Merged:\", len(df_merged), \"->\", merged_path)\n"]},{"cell_type":"markdown","metadata":{"id":"a-PhsNFnIlAT"},"source":["## 6) Dedup (PRISMA “duplicates removed”)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUiTRzqMIj_C"},"outputs":[],"source":["# ===== 6) Stage 3: dedup =====\n","\n","def pick_best_row(group: pd.DataFrame) -> pd.Series:\n","    \"\"\"\n","    Prefer rows with:\n","    - DOI present\n","    - abstract present\n","    - venue present\n","    - url present\n","    Tie-breaker: SemanticScholar > DBLP (often richer metadata)\n","    \"\"\"\n","    def score(row):\n","        s = 0\n","        s += 5 if pd.notna(row.get(\"doi\")) else 0\n","        s += 3 if pd.notna(row.get(\"abstract\")) else 0\n","        s += 2 if pd.notna(row.get(\"venue\")) else 0\n","        s += 1 if pd.notna(row.get(\"url\")) else 0\n","        s += 1 if row.get(\"source\") == \"SemanticScholar\" else 0\n","        return s\n","    idx = group.apply(score, axis=1).sort_values(ascending=False).index[0]\n","    return group.loc[idx]\n","\n","# primary\n","dedup_primary = df_merged.groupby(\"paper_id\", as_index=False).apply(pick_best_row).reset_index(drop=True)\n","\n","# secondary fallback for missing identifiers: title+year collisions\n","dedup_primary[\"title_norm\"] = dedup_primary[\"title\"].fillna(\"\").apply(normalize_title)\n","dedup_primary[\"ty_key\"] = dedup_primary.apply(\n","    lambda r: f\"{r['title_norm']}::{int(r['year']) if pd.notna(r['year']) else ''}\",\n","    axis=1\n",")\n","\n","dedup_final = dedup_primary.groupby(\"ty_key\", as_index=False).apply(pick_best_row).reset_index(drop=True)\n","dedup_final = dedup_final.drop(columns=[\"title_norm\",\"ty_key\"])\n","\n","dedup_path = DATA_PROCESSED / \"candidates_dedup.csv\"\n","dedup_final.to_csv(dedup_path, index=False)\n","print(\"Deduped:\", len(dedup_final), \"->\", dedup_path)\n"]},{"cell_type":"markdown","metadata":{"id":"7Xc0Lu6WO_FA"},"source":["## 7) Screening + taxonomy templates (manual step, but generated automatically)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MFZfAhG0O-Z6"},"outputs":[],"source":["# ===== 7) Stage 4: screening + taxonomy templates =====\n","\n","def export_screening_template(df: pd.DataFrame, path: Path) -> None:\n","    cols = [\"paper_id\",\"title\",\"year\",\"venue\",\"url\",\"source\"]\n","    out = df[cols].copy()\n","    out[\"decision\"] = \"\"   # include / exclude / maybe\n","    out[\"reason\"] = \"\"     # e.g., not NS, not design/manufacturing, no method detail, etc.\n","    out[\"notes\"] = \"\"\n","    out.to_csv(path, index=False)\n","\n","def export_taxonomy_template(df: pd.DataFrame, path: Path) -> None:\n","    cols = [\"paper_id\",\"title\",\"year\",\"venue\",\"url\"]\n","    out = df[cols].copy()\n","    out[\"kautz_type\"] = \"\"     # I..VI\n","    out[\"Dom\"] = \"\"            # CAD/Mfg/Asm/Mat/Topo/G/Dsgn\n","    out[\"NF\"] = \"\"             # P/Gen/Opt/S\n","    out[\"SS\"] = \"\"             # O/K/L/C/R\n","    out[\"constraint_prior\"] = \"\"\n","    out[\"dataset_benchmark\"] = \"\"\n","    out[\"key_metrics\"] = \"\"\n","    out.to_csv(path, index=False)\n","\n","screening_path = SCREENING_DIR / \"screening_log.csv\"\n","taxonomy_path = SCREENING_DIR / \"taxonomy_labels.csv\"\n","\n","export_screening_template(dedup_final, screening_path)\n","export_taxonomy_template(dedup_final, taxonomy_path)\n","\n","print(\"Wrote:\", screening_path)\n","print(\"Wrote:\", taxonomy_path)\n"]},{"cell_type":"markdown","source":["## 8) Plots (years + venues) → outputs/figures/"],"metadata":{"id":"NmuzlMf06bJO"}},{"cell_type":"code","source":["# ===== 8) Plots =====\n","\n","def plot_year_hist(df: pd.DataFrame, out_path: Path) -> None:\n","    years = df[\"year\"].dropna().astype(int)\n","    if years.empty:\n","        print(\"No years to plot.\")\n","        return\n","    plt.figure(figsize=(10,6))\n","    plt.hist(years, bins=min(30, max(5, years.nunique())))\n","    plt.xlabel(\"Year\")\n","    plt.ylabel(\"Count\")\n","    plt.title(\"Candidate papers by year (deduped)\")\n","    plt.tight_layout()\n","    plt.savefig(out_path, format=\"svg\")\n","    plt.close()\n","\n","def plot_top_venues(df: pd.DataFrame, out_path: Path, top_n: int = 25) -> None:\n","    v = df[\"venue\"].dropna()\n","    if v.empty:\n","        print(\"No venues to plot.\")\n","        return\n","    counts = v.value_counts().head(top_n)\n","    plt.figure(figsize=(12,6))\n","    plt.bar(counts.index.astype(str), counts.values)\n","    plt.xticks(rotation=75, ha=\"right\")\n","    plt.xlabel(\"Venue\")\n","    plt.ylabel(\"Count\")\n","    plt.title(f\"Top {top_n} venues (deduped candidates)\")\n","    plt.tight_layout()\n","    plt.savefig(out_path, format=\"svg\")\n","    plt.close()\n","\n","plot_year_hist(dedup_final, OUTPUTS_FIG / \"candidates_by_year.svg\")\n","plot_top_venues(dedup_final, OUTPUTS_FIG / \"top_venues.svg\", top_n=25)\n","\n","print(\"Saved figures to\", OUTPUTS_FIG)\n"],"metadata":{"id":"PT2WJVcD6a2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9) Export BibTeX bundle (only from DBLP rows that have it)"],"metadata":{"id":"d5OoWL07rPE8"}},{"cell_type":"code","source":["# ===== 9) Optional: BibTeX export =====\n","\n","def export_bibtex_from_dblp(df: pd.DataFrame, out_path: Path) -> None:\n","    bibs = df[\"dblp_bibtex\"].dropna().astype(str)\n","    bibs = [b.strip() for b in bibs if b.strip().startswith(\"@\")]\n","    # basic dedup by hash\n","    seen = set()\n","    uniq = []\n","    for b in bibs:\n","        h = hashlib.sha1(b.encode(\"utf-8\")).hexdigest()\n","        if h not in seen:\n","            seen.add(h)\n","            uniq.append(b)\n","    out_path.write_text(\"\\n\\n\".join(uniq) + \"\\n\", encoding=\"utf-8\")\n","    print(f\"BibTeX entries written: {len(uniq)} -> {out_path}\")\n","\n","export_bibtex_from_dblp(df_merged, BIB_DIR / \"candidates_from_dblp.bib\")\n"],"metadata":{"id":"ge6Fh83xrN3p"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNw3Di6LtszYxw93qQWjOfQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}